{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarianoChic09/MSc-ORT-Deep-Learning/blob/main/Obligatorio/obligatorio_deep_learning_2023_c_WANDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-IKUpIRO7_N"
      },
      "source": [
        "![AIRBNB](https://www.stevenridercpa.au/wp-content/uploads/2022/09/airbnb-tax.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvWFGvW6O7_P"
      },
      "source": [
        "# Obligatorio de Deep Learning\n",
        "## Semestre 2 - 2023\n",
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y230kyNjO7_P"
      },
      "source": [
        "## Problema\n",
        "\n",
        "Se presenta un dataset que contiene información de alojamientos publicados en AirBnB con sus respectivos precios. El tamaño del dataset de train es de 1.5 Gb aproximadamente, y 0.5 Gb el de test. Este cuenta con 84 variables predictoras que se podrán utilizar como consideren adecuado.\n",
        "\n",
        "El objetivo es asignar el precio correcto a los alojamientos listados.\n",
        "\n",
        "Además del dataset se les provee esta notebook conteniendo el script de carga de datos y un modelo baseline que corresponde a una arquitectura feed forward.\n",
        "\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0wbKLG_O7_Q"
      },
      "source": [
        "## Consigna\n",
        "\n",
        "### A) <u>Participación en Competencia Kaggle</u>:\n",
        "El objetivo de este punto es participar en la competencia de Kaggle y obtener como mínimo un Mean Absolute Error inferior a 70 puntos. [->Link a la competencia<-](https://www.kaggle.com/t/69c648e3aa214d1f812bf2314c8d4ffa).\n",
        "\n",
        "### B) <u>Utilización de Grid Search (o equivalente)</u>:\n",
        "Para cumplir con la busqueda de modelos óptimos se debe realizar un grid search lo más abarcativo y metódico posible. Recomendamos enfáticamente [Weights and Biases](https://wandb.ai/site)\n",
        "\n",
        "### C) <u>Se debe a su vez investigar e implementar las siguientes técnicas</u>:\n",
        "#### 1. [Batch Normalization](https://machinelearningmastery.com/how-to-accelerate-learning-of-deep-neural-networks-with-batch-normalization/)\n",
        "#### 2. [Gradient Normalization y/o Gradient Clipping](https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/)\n",
        "\n",
        "\n",
        "Además como en todas las tareas se evaluará la prolijidad de la entrega, el preprocesamiento de datos, visualizaciones y exploración de técnicas alternativas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iaoqOjjO7_Q"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TeJvT8iRPC4l",
        "outputId": "ba5e9e7c-3ac4-488c-865b-1e594c696a3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKSvbcSjO7_Q"
      },
      "source": [
        "## 1. Setup\n",
        "### 1.1 Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/Datasets"
      ],
      "metadata": {
        "id": "Ntv1OfCFPON4",
        "outputId": "172a745f-503d-4fa7-ff98-7db249a21c86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Whcv1sryO7_T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9pbFD39O7_U"
      },
      "source": [
        "### 1.2 Seteo de seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bxFQLSNzO7_U"
      },
      "outputs": [],
      "source": [
        "np.random.seed(117)\n",
        "tf.random.set_seed(117)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-GHRJXAO7_U"
      },
      "source": [
        "## 2. Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bqdM3sv4O7_U"
      },
      "outputs": [],
      "source": [
        "file_path = './obligatorio_DL/public_train_data.csv'\n",
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhKvdfZOO7_U"
      },
      "source": [
        "##  3. Análisis exploratorio de datos\n",
        "### 3.1 Dimensiones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XmmIb2VmO7_V",
        "outputId": "28749e00-912a-4a30-864d-a024a1779f8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(326287, 85)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R60VKhrQO7_V"
      },
      "source": [
        "### 3.2 Obtener información sobre las columnas y tipos de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2Lp-JkqZO7_V",
        "outputId": "24c13c48-88bd-411d-c92f-5c203da38a1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 326287 entries, 0 to 326286\n",
            "Data columns (total 85 columns):\n",
            " #   Column                          Non-Null Count   Dtype  \n",
            "---  ------                          --------------   -----  \n",
            " 0   id                              326287 non-null  int64  \n",
            " 1   Last Scraped                    326286 non-null  object \n",
            " 2   Name                            326018 non-null  object \n",
            " 3   Summary                         315651 non-null  object \n",
            " 4   Space                           228792 non-null  object \n",
            " 5   Description                     326188 non-null  object \n",
            " 6   Experiences Offered             326287 non-null  object \n",
            " 7   Neighborhood Overview           192513 non-null  object \n",
            " 8   Notes                           130729 non-null  object \n",
            " 9   Transit                         200649 non-null  object \n",
            " 10  Access                          177108 non-null  object \n",
            " 11  Interaction                     169193 non-null  object \n",
            " 12  House Rules                     195763 non-null  object \n",
            " 13  Thumbnail Url                   264638 non-null  object \n",
            " 14  Medium Url                      264637 non-null  object \n",
            " 15  Picture Url                     325786 non-null  object \n",
            " 16  XL Picture Url                  264638 non-null  object \n",
            " 17  Host ID                         326287 non-null  int64  \n",
            " 18  Host URL                        326287 non-null  object \n",
            " 19  Host Name                       325970 non-null  object \n",
            " 20  Host Since                      325970 non-null  object \n",
            " 21  Host Location                   324801 non-null  object \n",
            " 22  Host About                      195571 non-null  object \n",
            " 23  Host Response Time              250846 non-null  object \n",
            " 24  Host Response Rate              250845 non-null  float64\n",
            " 25  Host Acceptance Rate            27630 non-null   object \n",
            " 26  Host Thumbnail Url              325971 non-null  object \n",
            " 27  Host Picture Url                325971 non-null  object \n",
            " 28  Host Neighbourhood              244207 non-null  object \n",
            " 29  Host Listings Count             325971 non-null  float64\n",
            " 30  Host Total Listings Count       325970 non-null  float64\n",
            " 31  Host Verifications              325751 non-null  object \n",
            " 32  Street                          326287 non-null  object \n",
            " 33  Neighbourhood                   227447 non-null  object \n",
            " 34  Neighbourhood Cleansed          326287 non-null  object \n",
            " 35  Neighbourhood Group Cleansed    68297 non-null   object \n",
            " 36  City                            326004 non-null  object \n",
            " 37  State                           294992 non-null  object \n",
            " 38  Zipcode                         313815 non-null  object \n",
            " 39  Market                          322672 non-null  object \n",
            " 40  Smart Location                  326286 non-null  object \n",
            " 41  Country Code                    326286 non-null  object \n",
            " 42  Country                         326286 non-null  object \n",
            " 43  Latitude                        326287 non-null  float64\n",
            " 44  Longitude                       326287 non-null  float64\n",
            " 45  Property Type                   326280 non-null  object \n",
            " 46  Room Type                       326287 non-null  object \n",
            " 47  Accommodates                    326244 non-null  float64\n",
            " 48  Bathrooms                       325300 non-null  float64\n",
            " 49  Bedrooms                        325873 non-null  float64\n",
            " 50  Beds                            325693 non-null  float64\n",
            " 51  Bed Type                        326287 non-null  object \n",
            " 52  Amenities                       323399 non-null  object \n",
            " 53  Square Feet                     7993 non-null    float64\n",
            " 54  Security Deposit                136142 non-null  float64\n",
            " 55  Cleaning Fee                    208181 non-null  float64\n",
            " 56  Guests Included                 326286 non-null  float64\n",
            " 57  Extra People                    326274 non-null  float64\n",
            " 58  Minimum Nights                  326286 non-null  float64\n",
            " 59  Maximum Nights                  326286 non-null  float64\n",
            " 60  Calendar Updated                326287 non-null  object \n",
            " 61  Has Availability                6249 non-null    object \n",
            " 62  Availability 30                 326286 non-null  float64\n",
            " 63  Availability 60                 326286 non-null  float64\n",
            " 64  Availability 90                 326286 non-null  float64\n",
            " 65  Availability 365                326286 non-null  float64\n",
            " 66  Calendar last Scraped           326286 non-null  object \n",
            " 67  Number of Reviews               326286 non-null  float64\n",
            " 68  First Review                    246983 non-null  object \n",
            " 69  Last Review                     247046 non-null  object \n",
            " 70  Review Scores Rating            243160 non-null  float64\n",
            " 71  Review Scores Accuracy          242584 non-null  float64\n",
            " 72  Review Scores Cleanliness       242732 non-null  float64\n",
            " 73  Review Scores Checkin           242378 non-null  float64\n",
            " 74  Review Scores Communication     242710 non-null  float64\n",
            " 75  Review Scores Location          242423 non-null  float64\n",
            " 76  Review Scores Value             242347 non-null  float64\n",
            " 77  License                         9804 non-null    object \n",
            " 78  Jurisdiction Names              89415 non-null   object \n",
            " 79  Cancellation Policy             326286 non-null  object \n",
            " 80  Calculated host listings count  325689 non-null  float64\n",
            " 81  Reviews per Month               246983 non-null  float64\n",
            " 82  Geolocation                     326287 non-null  object \n",
            " 83  Features                        326092 non-null  object \n",
            " 84  Price                           326287 non-null  float64\n",
            "dtypes: float64(31), int64(2), object(52)\n",
            "memory usage: 211.6+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAdII-pqO7_V"
      },
      "source": [
        "### 3.3 Visualizar las primeras filas del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KtBuUl1TO7_V",
        "outputId": "aa1635a8-2117-4e99-bfaf-0188c14b1f82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id Last Scraped                                         Name  \\\n",
              "0   0   2017-05-12  Grand Loft in the heart of historic Antwerp   \n",
              "1   1   2017-05-03             CHARMING, CLEAN & COZY BUNGALOW!   \n",
              "2   2   2017-05-09                          la casa di maurizio   \n",
              "\n",
              "                                             Summary  \\\n",
              "0  Best location for visiting Antwerp!! Beautiful...   \n",
              "1  Very centrally located and less than 15 min fr...   \n",
              "2  nice apartment with view to via veneto , very ...   \n",
              "\n",
              "                                               Space  \\\n",
              "0  Welcome in Antwerp!! The loft is situated on t...   \n",
              "1       Well lit, private entrance with small patio.   \n",
              "2                                                NaN   \n",
              "\n",
              "                                         Description Experiences Offered  \\\n",
              "0  Best location for visiting Antwerp!! Beautiful...                none   \n",
              "1  Very centrally located and less than 15 min fr...                none   \n",
              "2  nice apartment with view to via veneto , very ...                none   \n",
              "\n",
              "                          Neighborhood Overview  \\\n",
              "0                                           NaN   \n",
              "1  Quiet. Pretty tree lined streets, safe area.   \n",
              "2                                           NaN   \n",
              "\n",
              "                                        Notes  \\\n",
              "0                                         NaN   \n",
              "1  Has dining table and high back desk chair.   \n",
              "2                                         NaN   \n",
              "\n",
              "                                             Transit  ...  \\\n",
              "0                                                NaN  ...   \n",
              "1  Uber, bus line and metro link is less than 5 m...  ...   \n",
              "2                                                NaN  ...   \n",
              "\n",
              "  Review Scores Location Review Scores Value License       Jurisdiction Names  \\\n",
              "0                   10.0                 9.0     NaN                      NaN   \n",
              "1                    NaN                 NaN     NaN  City of Los Angeles, CA   \n",
              "2                    NaN                 NaN     NaN                      NaN   \n",
              "\n",
              "  Cancellation Policy Calculated host listings count Reviews per Month  \\\n",
              "0              strict                            2.0               2.6   \n",
              "1            flexible                            1.0               NaN   \n",
              "2        flexible_new                            1.0               NaN   \n",
              "\n",
              "                             Geolocation  \\\n",
              "0  51.21938762207894, 4.4034442505151885   \n",
              "1  34.1892692286356, -118.41993491931177   \n",
              "2  41.90859623057272, 12.493518028459327   \n",
              "\n",
              "                                 Features  Price  \n",
              "0   Host Has Profile Pic,Instant Bookable  159.0  \n",
              "1  Host Has Profile Pic,Is Location Exact   49.0  \n",
              "2  Host Has Profile Pic,Is Location Exact   75.0  \n",
              "\n",
              "[3 rows x 85 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-45fb1081-e602-4b03-9e64-ef4d7d7edf73\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Last Scraped</th>\n",
              "      <th>Name</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Space</th>\n",
              "      <th>Description</th>\n",
              "      <th>Experiences Offered</th>\n",
              "      <th>Neighborhood Overview</th>\n",
              "      <th>Notes</th>\n",
              "      <th>Transit</th>\n",
              "      <th>...</th>\n",
              "      <th>Review Scores Location</th>\n",
              "      <th>Review Scores Value</th>\n",
              "      <th>License</th>\n",
              "      <th>Jurisdiction Names</th>\n",
              "      <th>Cancellation Policy</th>\n",
              "      <th>Calculated host listings count</th>\n",
              "      <th>Reviews per Month</th>\n",
              "      <th>Geolocation</th>\n",
              "      <th>Features</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2017-05-12</td>\n",
              "      <td>Grand Loft in the heart of historic Antwerp</td>\n",
              "      <td>Best location for visiting Antwerp!! Beautiful...</td>\n",
              "      <td>Welcome in Antwerp!! The loft is situated on t...</td>\n",
              "      <td>Best location for visiting Antwerp!! Beautiful...</td>\n",
              "      <td>none</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>strict</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>51.21938762207894, 4.4034442505151885</td>\n",
              "      <td>Host Has Profile Pic,Instant Bookable</td>\n",
              "      <td>159.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2017-05-03</td>\n",
              "      <td>CHARMING, CLEAN &amp; COZY BUNGALOW!</td>\n",
              "      <td>Very centrally located and less than 15 min fr...</td>\n",
              "      <td>Well lit, private entrance with small patio.</td>\n",
              "      <td>Very centrally located and less than 15 min fr...</td>\n",
              "      <td>none</td>\n",
              "      <td>Quiet. Pretty tree lined streets, safe area.</td>\n",
              "      <td>Has dining table and high back desk chair.</td>\n",
              "      <td>Uber, bus line and metro link is less than 5 m...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>City of Los Angeles, CA</td>\n",
              "      <td>flexible</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>34.1892692286356, -118.41993491931177</td>\n",
              "      <td>Host Has Profile Pic,Is Location Exact</td>\n",
              "      <td>49.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2017-05-09</td>\n",
              "      <td>la casa di maurizio</td>\n",
              "      <td>nice apartment with view to via veneto , very ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>nice apartment with view to via veneto , very ...</td>\n",
              "      <td>none</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>flexible_new</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>41.90859623057272, 12.493518028459327</td>\n",
              "      <td>Host Has Profile Pic,Is Location Exact</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 85 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45fb1081-e602-4b03-9e64-ef4d7d7edf73')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-45fb1081-e602-4b03-9e64-ef4d7d7edf73 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-45fb1081-e602-4b03-9e64-ef4d7d7edf73');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a4934f9b-60c4-4166-9924-bda4fea24cd3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a4934f9b-60c4-4166-9924-bda4fea24cd3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a4934f9b-60c4-4166-9924-bda4fea24cd3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBWWhtzoO7_V"
      },
      "source": [
        "### 3.4 Estadísticas descriptivas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0qEeGOEMO7_V",
        "outputId": "7ca257e3-9c9b-419f-d668-8390789bbbe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  id       Host ID  Host Response Rate  Host Listings Count  \\\n",
              "count  326287.000000  3.262870e+05       250845.000000        325971.000000   \n",
              "mean   163143.000000  3.236757e+07           93.408264             9.586000   \n",
              "std     94191.087979  3.174572e+07           17.536835            57.399711   \n",
              "min         0.000000  1.900000e+01            0.000000             0.000000   \n",
              "25%     81571.500000  6.869780e+06           98.000000             1.000000   \n",
              "50%    163143.000000  2.186737e+07          100.000000             1.000000   \n",
              "75%    244714.500000  4.799166e+07          100.000000             3.000000   \n",
              "max    326286.000000  1.350885e+08          100.000000          1114.000000   \n",
              "\n",
              "       Host Total Listings Count       Latitude      Longitude   Accommodates  \\\n",
              "count              325970.000000  326287.000000  326287.000000  326244.000000   \n",
              "mean                    9.586026      38.042816     -15.323924       3.270764   \n",
              "std                    57.399797      22.910029      70.101677       2.037446   \n",
              "min                     0.000000     -38.224427    -123.218712       1.000000   \n",
              "25%                     1.000000      38.923154     -73.968081       2.000000   \n",
              "50%                     1.000000      42.304549       0.090277       2.000000   \n",
              "75%                     3.000000      50.863658      12.342749       4.000000   \n",
              "max                  1114.000000      55.994889     153.637837      18.000000   \n",
              "\n",
              "           Bathrooms       Bedrooms  ...  Review Scores Rating  \\\n",
              "count  325300.000000  325873.000000  ...         243160.000000   \n",
              "mean        1.239482       1.358072  ...             92.880063   \n",
              "std         0.574784       0.921763  ...              8.569521   \n",
              "min         0.000000       0.000000  ...             20.000000   \n",
              "25%         1.000000       1.000000  ...             90.000000   \n",
              "50%         1.000000       1.000000  ...             95.000000   \n",
              "75%         1.000000       2.000000  ...            100.000000   \n",
              "max         8.000000      96.000000  ...            100.000000   \n",
              "\n",
              "       Review Scores Accuracy  Review Scores Cleanliness  \\\n",
              "count           242584.000000              242732.000000   \n",
              "mean                 9.524713                   9.326067   \n",
              "std                  0.855361                   1.038858   \n",
              "min                  2.000000                   2.000000   \n",
              "25%                  9.000000                   9.000000   \n",
              "50%                 10.000000                  10.000000   \n",
              "75%                 10.000000                  10.000000   \n",
              "max                 10.000000                  10.000000   \n",
              "\n",
              "       Review Scores Checkin  Review Scores Communication  \\\n",
              "count          242378.000000                242710.000000   \n",
              "mean                9.691416                     9.708253   \n",
              "std                 0.731702                     0.723143   \n",
              "min                 2.000000                     2.000000   \n",
              "25%                10.000000                    10.000000   \n",
              "50%                10.000000                    10.000000   \n",
              "75%                10.000000                    10.000000   \n",
              "max                10.000000                    10.000000   \n",
              "\n",
              "       Review Scores Location  Review Scores Value  \\\n",
              "count           242423.000000        242347.000000   \n",
              "mean                 9.468215             9.321031   \n",
              "std                  0.805116             0.906478   \n",
              "min                  2.000000             2.000000   \n",
              "25%                  9.000000             9.000000   \n",
              "50%                 10.000000            10.000000   \n",
              "75%                 10.000000            10.000000   \n",
              "max                 10.000000            10.000000   \n",
              "\n",
              "       Calculated host listings count  Reviews per Month          Price  \n",
              "count                   325689.000000      246983.000000  326287.000000  \n",
              "mean                         6.881531           1.486211     138.229041  \n",
              "std                         42.025986           1.752082     149.790527  \n",
              "min                          1.000000           0.010000       0.000000  \n",
              "25%                          1.000000           0.320000      55.000000  \n",
              "50%                          1.000000           0.890000      90.000000  \n",
              "75%                          2.000000           2.040000     150.000000  \n",
              "max                        752.000000         223.000000     999.000000  \n",
              "\n",
              "[8 rows x 33 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d471bd7d-00e7-4b3c-8b71-8c1623920c7d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Host ID</th>\n",
              "      <th>Host Response Rate</th>\n",
              "      <th>Host Listings Count</th>\n",
              "      <th>Host Total Listings Count</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>Accommodates</th>\n",
              "      <th>Bathrooms</th>\n",
              "      <th>Bedrooms</th>\n",
              "      <th>...</th>\n",
              "      <th>Review Scores Rating</th>\n",
              "      <th>Review Scores Accuracy</th>\n",
              "      <th>Review Scores Cleanliness</th>\n",
              "      <th>Review Scores Checkin</th>\n",
              "      <th>Review Scores Communication</th>\n",
              "      <th>Review Scores Location</th>\n",
              "      <th>Review Scores Value</th>\n",
              "      <th>Calculated host listings count</th>\n",
              "      <th>Reviews per Month</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>326287.000000</td>\n",
              "      <td>3.262870e+05</td>\n",
              "      <td>250845.000000</td>\n",
              "      <td>325971.000000</td>\n",
              "      <td>325970.000000</td>\n",
              "      <td>326287.000000</td>\n",
              "      <td>326287.000000</td>\n",
              "      <td>326244.000000</td>\n",
              "      <td>325300.000000</td>\n",
              "      <td>325873.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>243160.000000</td>\n",
              "      <td>242584.000000</td>\n",
              "      <td>242732.000000</td>\n",
              "      <td>242378.000000</td>\n",
              "      <td>242710.000000</td>\n",
              "      <td>242423.000000</td>\n",
              "      <td>242347.000000</td>\n",
              "      <td>325689.000000</td>\n",
              "      <td>246983.000000</td>\n",
              "      <td>326287.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>163143.000000</td>\n",
              "      <td>3.236757e+07</td>\n",
              "      <td>93.408264</td>\n",
              "      <td>9.586000</td>\n",
              "      <td>9.586026</td>\n",
              "      <td>38.042816</td>\n",
              "      <td>-15.323924</td>\n",
              "      <td>3.270764</td>\n",
              "      <td>1.239482</td>\n",
              "      <td>1.358072</td>\n",
              "      <td>...</td>\n",
              "      <td>92.880063</td>\n",
              "      <td>9.524713</td>\n",
              "      <td>9.326067</td>\n",
              "      <td>9.691416</td>\n",
              "      <td>9.708253</td>\n",
              "      <td>9.468215</td>\n",
              "      <td>9.321031</td>\n",
              "      <td>6.881531</td>\n",
              "      <td>1.486211</td>\n",
              "      <td>138.229041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>94191.087979</td>\n",
              "      <td>3.174572e+07</td>\n",
              "      <td>17.536835</td>\n",
              "      <td>57.399711</td>\n",
              "      <td>57.399797</td>\n",
              "      <td>22.910029</td>\n",
              "      <td>70.101677</td>\n",
              "      <td>2.037446</td>\n",
              "      <td>0.574784</td>\n",
              "      <td>0.921763</td>\n",
              "      <td>...</td>\n",
              "      <td>8.569521</td>\n",
              "      <td>0.855361</td>\n",
              "      <td>1.038858</td>\n",
              "      <td>0.731702</td>\n",
              "      <td>0.723143</td>\n",
              "      <td>0.805116</td>\n",
              "      <td>0.906478</td>\n",
              "      <td>42.025986</td>\n",
              "      <td>1.752082</td>\n",
              "      <td>149.790527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.900000e+01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-38.224427</td>\n",
              "      <td>-123.218712</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>81571.500000</td>\n",
              "      <td>6.869780e+06</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>38.923154</td>\n",
              "      <td>-73.968081</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>55.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>163143.000000</td>\n",
              "      <td>2.186737e+07</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>42.304549</td>\n",
              "      <td>0.090277</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>244714.500000</td>\n",
              "      <td>4.799166e+07</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>50.863658</td>\n",
              "      <td>12.342749</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.040000</td>\n",
              "      <td>150.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>326286.000000</td>\n",
              "      <td>1.350885e+08</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>1114.000000</td>\n",
              "      <td>1114.000000</td>\n",
              "      <td>55.994889</td>\n",
              "      <td>153.637837</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>752.000000</td>\n",
              "      <td>223.000000</td>\n",
              "      <td>999.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 33 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d471bd7d-00e7-4b3c-8b71-8c1623920c7d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d471bd7d-00e7-4b3c-8b71-8c1623920c7d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d471bd7d-00e7-4b3c-8b71-8c1623920c7d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-014d9a6c-7c7d-4f63-bfba-42d6ee167234\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-014d9a6c-7c7d-4f63-bfba-42d6ee167234')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-014d9a6c-7c7d-4f63-bfba-42d6ee167234 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LkP5LYdtO7_W",
        "outputId": "fc8fc5f1-d3d3-449d-de1b-23abefc53afa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'Last Scraped', 'Name', 'Summary', 'Space', 'Description',\n",
              "       'Experiences Offered', 'Neighborhood Overview', 'Notes', 'Transit',\n",
              "       'Access', 'Interaction', 'House Rules', 'Thumbnail Url', 'Medium Url',\n",
              "       'Picture Url', 'XL Picture Url', 'Host ID', 'Host URL', 'Host Name',\n",
              "       'Host Since', 'Host Location', 'Host About', 'Host Response Time',\n",
              "       'Host Response Rate', 'Host Acceptance Rate', 'Host Thumbnail Url',\n",
              "       'Host Picture Url', 'Host Neighbourhood', 'Host Listings Count',\n",
              "       'Host Total Listings Count', 'Host Verifications', 'Street',\n",
              "       'Neighbourhood', 'Neighbourhood Cleansed',\n",
              "       'Neighbourhood Group Cleansed', 'City', 'State', 'Zipcode', 'Market',\n",
              "       'Smart Location', 'Country Code', 'Country', 'Latitude', 'Longitude',\n",
              "       'Property Type', 'Room Type', 'Accommodates', 'Bathrooms', 'Bedrooms',\n",
              "       'Beds', 'Bed Type', 'Amenities', 'Square Feet', 'Security Deposit',\n",
              "       'Cleaning Fee', 'Guests Included', 'Extra People', 'Minimum Nights',\n",
              "       'Maximum Nights', 'Calendar Updated', 'Has Availability',\n",
              "       'Availability 30', 'Availability 60', 'Availability 90',\n",
              "       'Availability 365', 'Calendar last Scraped', 'Number of Reviews',\n",
              "       'First Review', 'Last Review', 'Review Scores Rating',\n",
              "       'Review Scores Accuracy', 'Review Scores Cleanliness',\n",
              "       'Review Scores Checkin', 'Review Scores Communication',\n",
              "       'Review Scores Location', 'Review Scores Value', 'License',\n",
              "       'Jurisdiction Names', 'Cancellation Policy',\n",
              "       'Calculated host listings count', 'Reviews per Month', 'Geolocation',\n",
              "       'Features', 'Price'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert Fin"
      ],
      "metadata": {
        "id": "KqfnLuO8t0fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Wedud4uTQtiZ",
        "outputId": "a959e3fa-2727-4543-adea-29874484f123",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(326287, 85)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columnas_con_NaNs_mayores_a_50_porciento = df.columns[df.isnull().sum() > 0.5*df.shape[0]]\n",
        "columnas_con_NaNs_mayores_a_50_porciento"
      ],
      "metadata": {
        "id": "39pwwdfNQnY-",
        "outputId": "24f9d820-4cb8-4173-b787-94098f345f7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Notes', 'Host Acceptance Rate', 'Neighbourhood Group Cleansed',\n",
              "       'Square Feet', 'Security Deposit', 'Has Availability', 'License',\n",
              "       'Jurisdiction Names'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zodYOpYRO7_W"
      },
      "source": [
        "## 4. Modelo Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq-FmVUKO7_W"
      },
      "source": [
        "### 4.1 Seleccionar características relevantes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drop_columns = list(columnas_con_NaNs_mayores_a_50_porciento)\n",
        "drop_columns.extend(['Host ID','Host URL','Price']) # Dropeo algunas columnas que no tienen sentido como el ID y el URL del host\n",
        "drop_columns"
      ],
      "metadata": {
        "id": "C0mY3hNkRw1y",
        "outputId": "a8e154fe-e4ae-47cc-b471-5bb381c93819",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Notes',\n",
              " 'Host Acceptance Rate',\n",
              " 'Neighbourhood Group Cleansed',\n",
              " 'Square Feet',\n",
              " 'Security Deposit',\n",
              " 'Has Availability',\n",
              " 'License',\n",
              " 'Jurisdiction Names',\n",
              " 'Host ID',\n",
              " 'Host URL',\n",
              " 'Price']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hM5ha5jvO7_W"
      },
      "outputs": [],
      "source": [
        "# features = ['Bathrooms', 'Bedrooms']  # Reemplaza con las características relevantes\n",
        "features = df.columns.drop(drop_columns)\n",
        "target = 'Price'\n",
        "df = df[[*features, target]]\n",
        "# df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "df_train = df.drop('Price',axis=1)\n",
        "numerical_cols = df_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "unique_counts = df_train.drop(numerical_cols,axis=1).nunique()\n",
        "\n",
        "# Calculo las filas que tienen una\n",
        "mean_string_length = df.apply(lambda col: col.dropna().astype(str).apply(len).mean())\n",
        "\n",
        "# Define los límites\n",
        "unique_value_limit = 20  # Por ejemplo, considera una columna categórica si tiene menos de 10 valores únicos\n",
        "string_length_limit = 20  # Por ejemplo, considera una columna de texto si la longitud media de la cadena es mayor que 20\n",
        "\n",
        "# Identifica las columnas categóricas y de texto\n",
        "categorical_cols = unique_counts[(unique_counts < unique_value_limit) & (mean_string_length < string_length_limit)].index.tolist()\n",
        "text_cols = unique_counts[(unique_counts >= unique_value_limit) | (mean_string_length >= string_length_limit)].index.tolist()\n",
        "\n",
        "# df['combined_text'] = df[text_cols].apply(lambda x: ' '.join(str(x)), axis=1)\n",
        "# text_data = df['combined_text']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop('Price', axis=1), df['Price'], test_size=0.2, random_state=0)\n",
        "\n",
        "# Now split the text data\n",
        "X_text_train = X_train.loc[:,text_cols ]\n",
        "X_text_test = X_test.loc[:,text_cols ]"
      ],
      "metadata": {
        "id": "l5I-JLyobGig"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "jTyCu5uhAuS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cada005-47cb-45aa-b53e-a452e2285bed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.34.0-py2.py3-none-any.whl (243 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.9/243.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.34.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the W&B Python Library and log into W&B\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "#Creamos un proyecto en WandB a través de su interfaz\n",
        "project = \"obligatorio_dl\"\n",
        "entity = \"marian-ai\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "p746tMf7_vsP",
        "outputId": "25fb9cf7-0607-4bee-92b5-d98eb3bd63e8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "fPVfmc2b8DAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc090c7e-2eda-49bc-cf86-d746e87b2da1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import dask.dataframe as dd\n",
        "# from dask.multiprocessing import get\n",
        "\n",
        "# dask_df = dd.from_pandas(X_text_train, npartitions=20)  # Partition dataframe\n",
        "# dask_result = dask_df.map_partitions(lambda df: df.applymap(preprocess_text)).compute(scheduler='multiprocessing')\n"
      ],
      "metadata": {
        "id": "HbmAmYNuBhCE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install fasttext"
      ],
      "metadata": {
        "id": "AvDIVfqERYOV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import fasttext\n",
        "# import fasttext.util\n",
        "\n",
        "# # Load the multilingual model (This will only load word vectors, not a full model)\n",
        "# ft = fasttext.load_model('wiki.multi.en.vec')\n",
        "\n",
        "# def get_embedding(text):\n",
        "#     # Tokenize the text into words\n",
        "#     words = text.split()\n",
        "#     # Get the vector for each word\n",
        "#     word_vectors = [ft.get_word_vector(word) for word in words]\n",
        "#     # Average the word vectors to get a sentence vector\n",
        "#     sentence_vector = np.mean(word_vectors, axis=0)\n",
        "#     return sentence_vector\n",
        "\n",
        "# # Example usage:\n",
        "# text = \"Bonjour, comment ça va ?\"  # French text\n",
        "# embedding = get_embedding(text)\n",
        "\n",
        "# text_es = \"Buenos dias como estas?\"\n",
        "# embedding_es = get_embedding(text_es)\n",
        "\n",
        "# print(text)\n",
        "# print(f\"El embedding es {embedding}\")\n",
        "\n",
        "# print(text_es)\n",
        "# print(f\"El embedding es {embedding_es}\")"
      ],
      "metadata": {
        "id": "iWiolJKGKDyy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # from joblib import Parallel, delayed\n",
        "\n",
        "# def preprocess_text(text):\n",
        "#     # If text is not a string, convert it to an empty string\n",
        "#     if not isinstance(text, str):\n",
        "#         text = ''\n",
        "\n",
        "#     def strip_html(text):\n",
        "#         soup = BeautifulSoup(text, \"html.parser\")\n",
        "#         return soup.get_text()\n",
        "\n",
        "#     def remove_between_square_brackets(text):\n",
        "#         return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#     def remove_special_characters(text):\n",
        "#         pattern = r'[^a-zA-z\\s]'\n",
        "#         text = re.sub(pattern, '', text)\n",
        "#         return text\n",
        "\n",
        "#     def remove_stop_words(text):\n",
        "#         stop_words = set(stopwords.words(\"english\"))\n",
        "#         tokens = text.split()\n",
        "#         tokens = [tok for tok in tokens if tok not in stop_words]\n",
        "#         return \" \".join(tokens)\n",
        "\n",
        "#     def lemmatize(text):\n",
        "#         wnl = WordNetLemmatizer()\n",
        "#         lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
        "#         return \" \".join(lemmas)\n",
        "\n",
        "#     text = strip_html(text)\n",
        "#     text = remove_between_square_brackets(text)\n",
        "#     text = remove_special_characters(text)\n",
        "#     text = text.lower()\n",
        "#     text = remove_stop_words(text)\n",
        "#     text = lemmatize(text)\n",
        "#     return text\n",
        "\n",
        "# def preprocess_column(col):\n",
        "#     return col.apply(preprocess_text)\n",
        "\n",
        "# # X_text_train[text_cols] = Parallel(n_jobs=-1)(delayed(preprocess_column)(X_text_train[col]) for col in text_cols)\n",
        "# # X_text_test[text_cols] = Parallel(n_jobs=-1)(delayed(preprocess_column)(X_text_test[col]) for col in text_cols)\n",
        "# for col in text_cols:\n",
        "#     X_text_train[col] = X_text_train[col].apply(preprocess_text)\n",
        "#     X_text_test[col] = X_text_test[col].apply(preprocess_text)\n",
        "\n",
        "# X_text_train['combined_text'] = X_text_train.apply(lambda x: ' '.join(x), axis=1)\n",
        "# X_text_test['combined_text'] = X_text_test.apply(lambda x: ' '.join(x), axis=1)\n"
      ],
      "metadata": {
        "id": "ir_EsTn17Ap8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from joblib import Parallel, delayed\n",
        "# import pandas as pd\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# import re\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# def preprocess_text(text):\n",
        "#     if not isinstance(text, str):\n",
        "#         return ''\n",
        "\n",
        "#     # Load resources within the function\n",
        "#     stop_words = set(stopwords.words(\"english\"))\n",
        "#     wnl = WordNetLemmatizer()\n",
        "\n",
        "#     def strip_html(text):\n",
        "#         soup = BeautifulSoup(text, \"html.parser\")\n",
        "#         return soup.get_text()\n",
        "\n",
        "#     def remove_special_characters(text):\n",
        "#         pattern = r'[^a-zA-Z\\s]'\n",
        "#         return re.sub(pattern, '', text)\n",
        "\n",
        "#     text = strip_html(text)\n",
        "#     text = remove_special_characters(text)\n",
        "#     text = text.lower()\n",
        "#     text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "#     text = ' '.join([wnl.lemmatize(word) for word in text.split()])\n",
        "\n",
        "#     return text\n",
        "\n",
        "# def preprocess_dataframe(df, columns):\n",
        "#     for col in columns:\n",
        "#         df[col] = df[col].apply(preprocess_text)\n",
        "#     return df\n",
        "# # Parallel processing\n",
        "# X_text_train = Parallel(n_jobs=-1)(delayed(preprocess_dataframe)(X_text_train, text_cols))\n",
        "# X_text_test = Parallel(n_jobs=-1)(delayed(preprocess_dataframe)(X_text_test, text_cols))\n",
        "\n",
        "# # Combining text columns\n",
        "# X_text_train['combined_text'] = X_text_train.apply(lambda x: ' '.join(x), axis=1)\n",
        "# X_text_test['combined_text'] = X_text_test.apply(lambda x: ' '.join(x), axis=1)"
      ],
      "metadata": {
        "id": "mLQMypD46vaO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def basic_cleaning(text):\n",
        "    # Remove HTML tags using BeautifulSoup\n",
        "    if not isinstance(text, str):\n",
        "         return ''\n",
        "\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # Correct encoding issues\n",
        "    text = text.replace(\"&amp;\", \"&\").replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\")\n",
        "\n",
        "    # Remove special characters or punctuation (customize regex as needed)\n",
        "    text = re.sub(r'[^a-zA-Z0-9.,!?/:;\\\"\\'\\s]', '', text)\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply to a text column in DataFrame\n",
        "X_text_train = X_text_train.applymap(basic_cleaning)\n",
        "X_text_test = X_text_test.applymap(basic_cleaning)\n",
        "\n",
        "\n",
        "X_text_train['combined_text'] = X_text_train.apply(lambda x: ' '.join(x), axis=1)\n",
        "X_text_test['combined_text'] = X_text_test.apply(lambda x: ' '.join(x), axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0ty-q7P8jCh",
        "outputId": "716e8788-ec00-41fa-c0e9-74e5349cbb31"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-160adc5ab63a>:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
            "<ipython-input-24-160adc5ab63a>:9: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLAMy9OJ_9-O",
        "outputId": "e3638849-0c6d-4831-cc58-a7bf0dc241bd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT tokenizer and model\n",
        "import tensorflow as tf\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased')\n",
        "seed = 42\n",
        "\n",
        "sample_size = 10000  # Adjust this to your computational capacity\n",
        "sampled_data = X_text_train.combined_text.sample(sample_size, random_state=seed)\n",
        "sampled_labels = y_train.loc[sampled_data.index]\n",
        "\n",
        "\n",
        "encoded_corpus = tokenizer(\n",
        "    text=sampled_data.tolist(),\n",
        "    add_special_tokens=True,\n",
        "    padding='max_length',\n",
        "    truncation='longest_first',\n",
        "    max_length=300,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "# encoded_corpus = tokenizer(\n",
        "#     text=X_text_train.combined_text.tolist(),\n",
        "#     add_special_tokens=True,\n",
        "#     padding='max_length',\n",
        "#     truncation='longest_first',\n",
        "#     max_length=300,\n",
        "#     return_attention_mask=True,\n",
        "#     return_tensors='tf'\n",
        "# )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAnHu08U02no",
        "outputId": "62df381c-1fbd-45d0-acfd-186d5cf56e92"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "labels = y_train.to_numpy()\n",
        "\n",
        "# Splitting the data\n",
        "test_size = 0.1\n",
        "# train_inputs, test_inputs, train_labels, test_labels = \\\n",
        "#     train_test_split(input_ids, labels, test_size=test_size, random_state=seed)\n",
        "# train_masks, test_masks, _, _ = train_test_split(attention_mask, labels, test_size=test_size, random_state=seed)\n",
        "\n",
        "# train_inputs, test_inputs, train_labels, test_labels = \\\n",
        "#     train_test_split(input_ids, sampled_labels.to_numpy(), test_size=test_size, random_state=seed)\n",
        "# train_masks, test_masks, _, _ = train_test_split(attention_mask, sampled_labels.to_numpy(), test_size=test_size, random_state=seed)\n",
        "\n",
        "\n",
        "# # Scaling the labels\n",
        "# price_scaler = StandardScaler()\n",
        "# price_scaler.fit(train_labels.reshape(-1, 1))\n",
        "# train_labels = price_scaler.transform(train_labels.reshape(-1, 1))\n",
        "# test_labels = price_scaler.transform(test_labels.reshape(-1, 1))\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Splitting indices for input_ids and attention_mask\n",
        "train_indices, test_indices = train_test_split(\n",
        "    range(len(sampled_labels)), test_size=test_size, random_state=seed)\n",
        "\n",
        "# Convert numpy indices to TensorFlow tensors\n",
        "train_indices = tf.convert_to_tensor(train_indices, dtype=tf.int32)\n",
        "test_indices = tf.convert_to_tensor(test_indices, dtype=tf.int32)\n",
        "\n",
        "# Using tf.gather to index input_ids and attention_mask\n",
        "train_inputs = tf.gather(input_ids, train_indices)\n",
        "test_inputs = tf.gather(input_ids, test_indices)\n",
        "train_masks = tf.gather(attention_mask, train_indices)\n",
        "test_masks = tf.gather(attention_mask, test_indices)\n",
        "\n",
        "# Splitting and scaling labels as before\n",
        "train_labels, test_labels = train_test_split(\n",
        "    sampled_labels.to_numpy(), test_size=test_size, random_state=seed)\n",
        "\n",
        "price_scaler = StandardScaler()\n",
        "price_scaler.fit(train_labels.reshape(-1, 1))\n",
        "train_labels = price_scaler.transform(train_labels.reshape(-1, 1))\n",
        "test_labels = price_scaler.transform(test_labels.reshape(-1, 1))\n",
        "\n",
        "# Define the model\n",
        "def build_model():\n",
        "    input_ids = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "    bert_output = bert_model([input_ids, attention_mask])\n",
        "    cls_token_output = bert_output.last_hidden_state[:, 0, :]\n",
        "    output = tf.keras.layers.Dense(1)(cls_token_output)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-8),\n",
        "                  loss=tf.keras.losses.MeanSquaredError(),\n",
        "                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x=[train_inputs, train_masks],\n",
        "    y=train_labels,\n",
        "    validation_data=([test_inputs, test_masks], test_labels),\n",
        "    batch_size=16,\n",
        "    epochs=5\n",
        ")\n",
        "\n",
        "# Evaluate and predict\n",
        "test_loss, test_mae = model.evaluate(x=[test_inputs, test_masks], y=test_labels)\n",
        "predictions = model.predict(x=[test_inputs, test_masks])\n",
        "\n",
        "# Rescale predictions\n",
        "y_pred = price_scaler.inverse_transform(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5K0zHvkS6wd",
        "outputId": "4b8d2bf5-6ae3-492a-9fad-956d03e90077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300/563 [==============>...............] - ETA: 4:47 - loss: 0.9497 - mean_absolute_error: 0.6384"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "texts = X_text_train['combined_text'].values\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(texts)\n"
      ],
      "metadata": {
        "id": "1mj3bt0n9hcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word2vec_model = KeyedVectors.load_word2vec_format('word2vec.bin', binary=True)\n"
      ],
      "metadata": {
        "id": "nDu2eZ139kGK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "000ffce6-ae49-4a10-a06e-89e2864d30c3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-7484ee681c9f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mword2vec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'word2vec.bin'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = word2vec_model.vector_size\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word2vec_model:\n",
        "        embedding_matrix[i] = word2vec_model[word]\n"
      ],
      "metadata": {
        "id": "axIO7wCV9n1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
        "\n",
        "# Assuming `embedding_matrix` is already defined as per your previous code\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=embedding_matrix.shape[0],\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=False  # Set to True if you want to fine-tune the embeddings\n",
        ")\n",
        "\n",
        "input_text = Input(shape=(None,), dtype='int32')\n",
        "embedded_text = embedding_layer(input_text)\n",
        "lstm_output, _, _ = LSTM(32, return_sequences=True, return_state=True)(embedded_text)\n",
        "output = Dense(2, activation='softmax')(lstm_output[:, -1, :])\n",
        "\n",
        "model = Model(inputs=input_text, outputs=output)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ArM6HrEe9_qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)])\n",
        "\n",
        "X_num_cat_train = preprocessor.fit_transform(X_train)\n",
        "X_num_cat_test = preprocessor.transform(X_test)\n"
      ],
      "metadata": {
        "id": "6S2VvbH47jbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df,imputer_strategy_numeric,imputer_strategy_categorical,text_tokenizer_num_words,text_max_length):\n",
        "    df_train = df.drop('Price', axis=1)\n",
        "    numerical_cols = df_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "    unique_counts = df_train.drop(numerical_cols, axis=1).nunique()\n",
        "    mean_string_length = df.apply(lambda col: col.dropna().astype(str).apply(len).mean())\n",
        "\n",
        "    unique_value_limit = 20\n",
        "    string_length_limit = 20\n",
        "\n",
        "    categorical_cols = unique_counts[(unique_counts < unique_value_limit) & (mean_string_length < string_length_limit)].index.tolist()\n",
        "    text_cols = unique_counts[(unique_counts >= unique_value_limit) | (mean_string_length >= string_length_limit)].index.tolist()\n",
        "\n",
        "    df['combined_text'] = df[text_cols].apply(lambda x: ' '.join(str(x)), axis=1)\n",
        "    text_data = df['combined_text']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df.drop('Price', axis=1), df['Price'], test_size=0.2, random_state=0)\n",
        "\n",
        "    X_text_train = X_train['combined_text']\n",
        "    X_text_test = X_test['combined_text']\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy=imputer_strategy_numeric)),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy=imputer_strategy_categorical, fill_value='missing')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)])\n",
        "\n",
        "    X_num_cat_train = preprocessor.fit_transform(X_train)\n",
        "    X_num_cat_test = preprocessor.transform(X_test)\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=text_tokenizer_num_words)\n",
        "    tokenizer.fit_on_texts(X_text_train)\n",
        "\n",
        "    max_length = text_max_length\n",
        "\n",
        "    X_text_train_padded = pad_sequences(tokenizer.texts_to_sequences(X_text_train), maxlen=max_length)\n",
        "    X_text_test_padded = pad_sequences(tokenizer.texts_to_sequences(X_text_test), maxlen=max_length)\n",
        "\n",
        "    return X_num_cat_train, X_text_train_padded, y_train, X_num_cat_test, X_text_test_padded, y_test\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Flatten, concatenate, Dropout\n",
        "\n",
        "def build_model(neurons, dropout, optimizer, text_tokenizer_num_words, embedding_output_dim, embedding_matrix):\n",
        "    # Text data input\n",
        "    text_input = Input(shape=(100,), name='text_input')\n",
        "    text_embedding = Embedding(\n",
        "        input_dim=text_tokenizer_num_words,\n",
        "        output_dim=embedding_output_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        trainable=True\n",
        "    )(text_input)\n",
        "    lstm_output, _, _ = LSTM(neurons[0], return_sequences=True, return_state=True)(text_embedding)\n",
        "\n",
        "    num_cat_input = Input(shape=(neurons[1],), name='num_cat_input')\n",
        "\n",
        "    combined_input = concatenate([lstm_output[:, -1, :], num_cat_input])\n",
        "\n",
        "    hidden_layer = Dense(neurons[2], activation='relu')(combined_input)\n",
        "    hidden_dropout = Dropout(dropout)(hidden_layer)\n",
        "\n",
        "    for n in neurons[3:]:\n",
        "        hidden_layer = Dense(n, activation='relu')(hidden_dropout)\n",
        "        hidden_dropout = Dropout(dropout)(hidden_layer)\n",
        "\n",
        "    # Output layer\n",
        "    output_layer = Dense(1)(hidden_dropout)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=[text_input, num_cat_input], outputs=output_layer)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# def build_model(neurons, dropout, optimizer,text_tokenizer_num_words,embedding_output_dim):\n",
        "#     text_input = Input(shape=(100,), name='text_input')\n",
        "#     num_cat_input = Input(shape=(neurons[0],), name='num_cat_input')  # assuming the first layer neuron count for input shape\n",
        "\n",
        "#     text_embedding = Embedding(input_dim=text_tokenizer_num_words, output_dim=embedding_output_dim)(text_input)\n",
        "#     text_flatten = Flatten()(text_embedding)\n",
        "\n",
        "#     combined_input = concatenate([text_flatten, num_cat_input])\n",
        "\n",
        "#     hidden_layer = Dense(neurons[0], activation='relu')(combined_input)\n",
        "#     hidden_dropout = Dropout(dropout)(hidden_layer)\n",
        "\n",
        "#     for n in neurons[1:]:\n",
        "#         hidden_layer = Dense(n, activation='relu')(hidden_dropout)\n",
        "#         hidden_dropout = Dropout(dropout)(hidden_layer)\n",
        "\n",
        "#     output_layer = Dense(1)(hidden_dropout)  # Assuming a regression task\n",
        "\n",
        "#     model = Model(inputs=[text_input, num_cat_input], outputs=output_layer)\n",
        "#     model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "#     return model\n"
      ],
      "metadata": {
        "id": "f5kKi1FuTCTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "sweep_config = {\n",
        "    'name': 'sweep_example',\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'val_loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'dropout': {\n",
        "            'value': 0.1\n",
        "        },\n",
        "        'neurons': {\n",
        "            'values': [[32, 2], [64, 32, 2]]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['adam', 'sgd']\n",
        "        },\n",
        "        'imputer_strategy_numeric': {\n",
        "            'values': ['mean', 'median', 'most_frequent', 'constant']\n",
        "        },\n",
        "        'imputer_strategy_categorical': {\n",
        "            'values': ['most_frequent', 'constant']\n",
        "        },\n",
        "        'text_tokenizer_num_words': {\n",
        "            'values': [5000, 10000, 15000]\n",
        "        },\n",
        "        'text_max_length': {\n",
        "            'values': [50, 100, 150]\n",
        "        },\n",
        "        'embedding_output_dim': {\n",
        "            'values': [8, 16, 32]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "pprint.pprint(sweep_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPVPZWmYTcOE",
        "outputId": "26586c4c-4659-4e11-cfeb-0ce940ccd001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'method': 'grid',\n",
            " 'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
            " 'name': 'sweep_example',\n",
            " 'parameters': {'dropout': {'value': 0.1},\n",
            "                'embedding_output_dim': {'values': [8, 16, 32]},\n",
            "                'imputer_strategy_categorical': {'values': ['most_frequent',\n",
            "                                                            'constant']},\n",
            "                'imputer_strategy_numeric': {'values': ['mean',\n",
            "                                                        'median',\n",
            "                                                        'most_frequent',\n",
            "                                                        'constant']},\n",
            "                'neurons': {'values': [[32, 2], [64, 32, 2]]},\n",
            "                'optimizer': {'values': ['adam', 'sgd']},\n",
            "                'text_max_length': {'values': [50, 100, 150]},\n",
            "                'text_tokenizer_num_words': {'values': [5000, 10000, 15000]}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout, Input, Embedding, Flatten, Dense, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def train():\n",
        "    with wandb.init() as run:\n",
        "        config = run.config\n",
        "        X_num_cat_train, X_text_train_padded, y_train, X_num_cat_test, X_text_test_padded, y_test = preprocess_data(df,config.imputer_strategy_numeric,config.imputer_strategy_categorical,config.text_tokenizer_num_words,config.text_max_length)\n",
        "        model = build_model(config.neurons, config.dropout, config.optimizer,config.embedding_output_dim)\n",
        "        wandb_callback = wandb.keras.WandbCallback()\n",
        "        model.fit([X_text_train_padded, X_num_cat_train], y_train, validation_data=([X_text_test_padded, X_num_cat_test], y_test), epochs=10, batch_size=32, callbacks=[wandb_callback])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1VlYhzTb_hMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=project, entity=entity)\n",
        "wandb.agent(sweep_id, function=train, count=10, project=project, entity=entity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "rPVjLnA0ZzUP",
        "outputId": "069336d2-2e53-436f-a709-940f92ec437e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: dnybgb90\n",
            "Sweep URL: https://wandb.ai/marian-ai/obligatorio_dl/sweeps/dnybgb90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eqlo005i with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_output_dim: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timputer_strategy_categorical: most_frequent\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timputer_strategy_numeric: mean\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tneurons: [32, 2]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttext_max_length: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttext_tokenizer_num_words: 5000\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmariano-chicatun\u001b[0m (\u001b[33mmarian-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/Colab Notebooks/Datasets/wandb/run-20231030_231007-eqlo005i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/marian-ai/obligatorio_dl/runs/eqlo005i' target=\"_blank\">celestial-sweep-1</a></strong> to <a href='https://wandb.ai/marian-ai/obligatorio_dl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/marian-ai/obligatorio_dl/sweeps/dnybgb90' target=\"_blank\">https://wandb.ai/marian-ai/obligatorio_dl/sweeps/dnybgb90</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/marian-ai/obligatorio_dl' target=\"_blank\">https://wandb.ai/marian-ai/obligatorio_dl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/marian-ai/obligatorio_dl/sweeps/dnybgb90' target=\"_blank\">https://wandb.ai/marian-ai/obligatorio_dl/sweeps/dnybgb90</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/marian-ai/obligatorio_dl/runs/eqlo005i' target=\"_blank\">https://wandb.ai/marian-ai/obligatorio_dl/runs/eqlo005i</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa previa"
      ],
      "metadata": {
        "id": "GvrjQnZicHzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Defino pipeline de preprocesamiento numerico\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Defino pipeline de preprocesamiento categorico\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Create a column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)])\n",
        "\n",
        "# Create a pipeline\n",
        "df_preprocessed = preprocessor.fit_transform(df)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_text_train)\n",
        "max_length = 100\n",
        "X_text_train_padded = pad_sequences(tokenizer.texts_to_sequences(X_text_train), maxlen=max_length)\n",
        "X_text_test_padded = pad_sequences(tokenizer.texts_to_sequences(X_text_test), maxlen=max_length)\n",
        "\n",
        "X_num_cat_train = preprocessor.fit_transform(X_train)\n",
        "X_num_cat_test = preprocessor.transform(X_test)"
      ],
      "metadata": {
        "id": "G56Z9oViXozk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa2c673-fea6-46e7-dba9-b8f285459c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-22-d8fce40df4a2>\", line 21, in <cell line: 21>\n",
            "    tokenizer.fit_on_texts(X_text_train)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/text.py\", line 293, in fit_on_texts\n",
            "    seq = text_to_word_sequence(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/text.py\", line 80, in text_to_word_sequence\n",
            "    seq = input_text.split(split)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 878, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 396, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, strict, {})\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 431, in _joinrealpath\n",
            "    st = os.lstat(newpath)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[text_cols].dtypes"
      ],
      "metadata": {
        "id": "7zDxLJlWYpfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preproceso la data de texto"
      ],
      "metadata": {
        "id": "EayJNGIvKjYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uno todos los datos"
      ],
      "metadata": {
        "id": "81NsMtldK3fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_text_train_padded), len(X_num_cat_train), len(y_train))\n",
        "print(np.any(np.isnan(X_text_train_padded)), np.any(np.isnan(X_num_cat_train)), np.any(np.isnan(y_train)))\n"
      ],
      "metadata": {
        "id": "c_xx3JdaeVOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Assume X_num_cat_train and X_num_cat_test are your numerical and categorical data split into training and test sets\n",
        "# X_num_cat_train, X_num_cat_test, y_train, y_test = train_test_split(df_preprocessed, df['Price'], test_size=0.2, random_state=0)\n",
        "\n",
        "# Separate input layers\n",
        "text_input = Input(shape=(100,), name='text_input')\n",
        "num_cat_input = Input(shape=(X_num_cat_train.shape[1],), name='num_cat_input')\n",
        "\n",
        "# Text data path\n",
        "text_embedding = Embedding(input_dim=5000, output_dim=16)(text_input)\n",
        "text_flatten = Flatten()(text_embedding)\n",
        "\n",
        "# Combine the processing paths\n",
        "combined_input = concatenate([text_flatten, num_cat_input])\n",
        "\n",
        "# Continue with your model\n",
        "hidden_layer = Dense(128, activation='relu')(combined_input)\n",
        "output_layer = Dense(1)(hidden_layer)  # Assuming a regression task\n",
        "\n",
        "model = Model(inputs=[text_input, num_cat_input], outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n"
      ],
      "metadata": {
        "id": "cXNKVTbLK52v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBPTKQutO7_W"
      },
      "source": [
        "### 4.2 Dividir los datos en conjuntos de entrenamiento y prueba"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Wva14b_yK1q-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSemNtYBO7_X"
      },
      "source": [
        "### 4.3 Definir el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z879IPx3O7_X"
      },
      "source": [
        "### 4.4 Entrenar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fU3vaIpO7_X"
      },
      "outputs": [],
      "source": [
        "history = model.fit([X_text_train_padded, X_num_cat_train], y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_UMDxP3O7_X"
      },
      "source": [
        "### 4.5 Evaluar en Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YsNjembO7_X"
      },
      "outputs": [],
      "source": [
        "loss, mae = model.evaluate([X_text_test_padded, X_num_cat_test], y_test, verbose=0)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test MAE: {mae}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P1lt_pIO7_Y"
      },
      "source": [
        "## 5 Generación de salida para competencia en Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhCrQhrpO7_Y"
      },
      "outputs": [],
      "source": [
        "file_path2 = './obligatorio_DL/private_data_to_predict.csv'\n",
        "data_for_kaggle = pd.read_csv(file_path2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text_cols.remove('combined_text')"
      ],
      "metadata": {
        "id": "WoDhbKOT3yTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_cols)"
      ],
      "metadata": {
        "id": "35hTeHUk4Ryw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_num_cat_kaggle = preprocessor.transform(data_for_kaggle)\n",
        "data_for_kaggle['all_text'] = data_for_kaggle[text_cols].apply(lambda x: ' '.join(str(x)), axis=1)\n",
        "\n",
        "X_text_kaggle_sequences = tokenizer.texts_to_sequences(data_for_kaggle['all_text'])\n",
        "X_text_kaggle_padded = pad_sequences(X_text_kaggle_sequences, maxlen=max_length)\n",
        "\n",
        "kaggle_results = model.predict([X_text_kaggle_padded, X_num_cat_kaggle])\n"
      ],
      "metadata": {
        "id": "TREpyPYF2nZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ids = data_for_kaggle['id']\n",
        "test_ids = np.array(test_ids).reshape(-1,1)\n",
        "output = np.stack((test_ids, kaggle_results), axis=-1)\n",
        "output = output.reshape([-1, 2])\n",
        "df = pd.DataFrame(output)\n",
        "df.columns = ['id','expected']\n",
        "df['expected'] = df['expected'].fillna(0)\n",
        "df.to_csv(\"output_to_submit.csv\", index = False, index_label = False)"
      ],
      "metadata": {
        "id": "LB1JIiCZ2wG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Con WandB"
      ],
      "metadata": {
        "id": "8Lgs9PaF_fNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "gdojPmdWt4OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "# sweep_config = {\n",
        "# 'name': 'sweep_example',\n",
        "# 'method': 'grid',\n",
        "# 'metric': {\n",
        "#     'name': 'val_loss',\n",
        "#     'goal': 'minimize'\n",
        "# },\n",
        "# 'parameters': {\n",
        "#     'dropout':{'value': 0.1},\n",
        "#     'neurons':{\n",
        "#         'values': [[32,2],[64,32,2]]\n",
        "#         },\n",
        "#     'optimizer': {\n",
        "#         'values': ['adam', 'sgd']\n",
        "#         }\n",
        "# }\n",
        "# }\n",
        "\n",
        "sweep_config = {\n",
        "    'name': 'sweep_example',\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'val_loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'dropout': {'value': 0.1},\n",
        "        'neurons': {'values': [[32, 2], [64, 32, 2]]},\n",
        "        'optimizer': {'values': ['adam', 'sgd']},\n",
        "        'imputer_strategy': {'values': ['mean', 'median', 'most_frequent']},  # Imputer strategies\n",
        "        'scaler': {'values': ['standard', 'minmax']},  # Scaler options: standard scaler or minmax scaler\n",
        "        'text_embedding_dim': {'values': [16, 32]},  # Embedding dimensions for text data\n",
        "    }\n",
        "}\n",
        "\n",
        "pprint.pprint(sweep_config)"
      ],
      "metadata": {
        "id": "nFt2vP6S_y-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import traceback\n",
        "# def run_train():\n",
        "#     try:\n",
        "#         with wandb.init(config=None, project=project, entity=entity):\n",
        "#             # initialize model\n",
        "#             config = wandb.config\n",
        "#             print(config)\n",
        "#             model= get_model(config.neurons, config.optimizer, config.dropout)\n",
        "#             tf.keras.backend.clear_session()\n",
        "#             wandb_callback = wandb.keras.WandbCallback()\n",
        "#             model.fit([X_text_train_padded, X_num_cat_train], y_train,\n",
        "#                       epochs=5, batch_size=128, validation_split=0.2,\n",
        "#                       callbacks=[wandb_callback], max_queue_size=3, workers=2)\n",
        "#     except Exception as e:\n",
        "#         # exit gracefully, so wandb logs the problem\n",
        "#         print(traceback.print_exc(), file=sys.stderr)\n",
        "#         exit(1)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "def run_train():\n",
        "    try:\n",
        "        with wandb.init(config=None, project=project, entity=entity):\n",
        "            # initialize model\n",
        "            config = wandb.config\n",
        "            print(config)\n",
        "\n",
        "            # Configure preprocessing based on sweep config\n",
        "            numeric_transformer = Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy=config.imputer_strategy)),\n",
        "                ('scaler', StandardScaler() if config.scaler == 'standard' else MinMaxScaler())])\n",
        "\n",
        "            preprocessor = ColumnTransformer(\n",
        "                transformers=[('num', numeric_transformer, numerical_cols)])\n",
        "\n",
        "            X_num_cat_train = preprocessor.fit_transform(X_train)\n",
        "            X_num_cat_test = preprocessor.transform(X_test)\n",
        "\n",
        "            model = get_model(config.neurons, config.optimizer, config.dropout, config.text_embedding_dim)\n",
        "            tf.keras.backend.clear_session()\n",
        "            wandb_callback = wandb.keras.WandbCallback()\n",
        "            model.fit([X_text_train_padded, X_num_cat_train], y_train,\n",
        "                      epochs=5, batch_size=128, validation_split=0.2,\n",
        "                      callbacks=[wandb_callback], max_queue_size=3, workers=2)\n",
        "\n",
        "    except Exception as e:\n",
        "        # exit gracefully, so wandb logs the problem\n",
        "        print(traceback.print_exc(), file=sys.stderr)\n",
        "        exit(1)"
      ],
      "metadata": {
        "id": "w00Vab8P_lMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=project, entity=entity)\n",
        "wandb.agent(sweep_id, function=run_train, count=10, project=project, entity=entity)\n"
      ],
      "metadata": {
        "id": "9TQXlNcS_mnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert Fine tuning"
      ],
      "metadata": {
        "id": "QrnitVg8t6hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cambiar a Keras"
      ],
      "metadata": {
        "id": "KImYBr5DuSEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def treat_euro(text):\n",
        "    text = re.sub(r'(euro[^s])|(euros)|(€)', ' euros', text)\n",
        "    return text\n",
        "def treat_m2(text):\n",
        "    text = re.sub(r'(m2)|(m²)', ' m²', text)\n",
        "    return text\n",
        "\n",
        "def filter_ibans(text):\n",
        "    pattern = r'fr\\d{2}[ ]\\d{4}[ ]\\d{4}[ ]\\d{4}[ ]\\d{4}[ ]\\d{2}|fr\\d{20}|fr[ ]\\d{2}[ ]\\d{3}[ ]\\d{3}[ ]\\d{3}[ ]\\d{5}'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def remove_space_between_numbers(text):\n",
        "    text = re.sub(r'(\\d)\\s+(\\d)', r'\\1\\2', text)\n",
        "    return text\n",
        "def filter_emails(text):\n",
        "    pattern = r'(?:(?!.*?[.]{2})[a-zA-Z0-9](?:[a-zA-Z0-9.+!%-]{1,64}|)|\\\"[a-zA-Z0-9.+!% -]{1,64}\\\")@[a-zA-Z0-9][a-zA-Z0-9.-]+(.[a-z]{2,}|.[0-9]{1,})'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def filter_ref(text):\n",
        "    pattern = r'(\\(*)(ref|réf)(\\.|[ ])\\d+(\\)*)'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def filter_websites(text):\n",
        "    pattern = r'(http\\:\\/\\/|https\\:\\/\\/)?([a-z0-9][a-z0-9\\-]*\\.)+[a-z][a-z\\-]*'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def filter_phone_numbers(text):\n",
        "    pattern = r'(?:(?:\\+|00)33[\\s.-]{0,3}(?:\\(0\\)[\\s.-]{0,3})?|0)[1-9](?:(?:[\\s.-]?\\d{2}){4}|\\d{2}(?:[\\s.-]?\\d{3}){2})|(\\d{2}[ ]\\d{2}[ ]\\d{3}[ ]\\d{3})'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace(u'\\xa0', u' ')\n",
        "    text = treat_m2(text)\n",
        "    text = treat_euro(text)\n",
        "    text = filter_phone_numbers(text)\n",
        "    text = filter_emails(text)\n",
        "    text = filter_ibans(text)\n",
        "    text = filter_ref(text)\n",
        "    text = filter_websites(text)\n",
        "    text = remove_space_between_numbers(text)\n",
        "    return text\n",
        "df['cleaned_description'] = df.description.apply(clean_text)\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "encoded_corpus = tokenizer(text=df.cleaned_description.tolist(),\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=300,\n",
        "                            return_attention_mask=True)\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "\n",
        "import numpy as np\n",
        "def filter_long_descriptions(tokenizer, descriptions, max_len):\n",
        "    indices = []\n",
        "    lengths = tokenizer(descriptions, padding=False,\n",
        "                     truncation=False, return_length=True)['length']\n",
        "    for i in range(len(descriptions)):\n",
        "        if lengths[i] <= max_len-2:\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "short_descriptions = filter_long_descriptions(tokenizer,\n",
        "                               df.cleaned_description.tolist(), 300)\n",
        "input_ids = np.array(input_ids)[short_descriptions]\n",
        "attention_mask = np.array(attention_mask)[short_descriptions]\n",
        "labels = df.prix.to_numpy()[short_descriptions]\n",
        "from sklearn.model_selection import train_test_split\n",
        "test_size = 0.1\n",
        "seed = 42\n",
        "train_inputs, test_inputs, train_labels, test_labels = \\\n",
        "            train_test_split(input_ids, labels, test_size=test_size,\n",
        "                             random_state=seed)\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_mask,\n",
        "                                        labels, test_size=test_size,\n",
        "                                        random_state=seed)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "price_scaler = StandardScaler()\n",
        "price_scaler.fit(train_labels.reshape(-1, 1))\n",
        "train_labels = price_scaler.transform(train_labels.reshape(-1, 1))\n",
        "test_labels = price_scaler.transform(test_labels.reshape(-1, 1))\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "batch_size = 32\n",
        "def create_dataloaders(inputs, masks, labels, batch_size):\n",
        "    input_tensor = torch.tensor(inputs)\n",
        "    mask_tensor = torch.tensor(masks)\n",
        "    labels_tensor = torch.tensor(labels)\n",
        "    dataset = TensorDataset(input_tensor, mask_tensor,\n",
        "                            labels_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size,\n",
        "                            shuffle=True)\n",
        "    return dataloader\n",
        "train_dataloader = create_dataloaders(train_inputs, train_masks,\n",
        "                                      train_labels, batch_size)\n",
        "test_dataloader = create_dataloaders(test_inputs, test_masks,\n",
        "                                     test_labels, batch_size)\n",
        "import torch.nn as nn\n",
        "class BertRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self, drop_rate=0.2, freeze_bert=False):\n",
        "\n",
        "        super(BertRegressor, self).__init__()\n",
        "        D_in, D_out = 768, 1\n",
        "\n",
        "        self.bert = \\\n",
        "                   CamembertModel.from_pretrained('bert-base-multilingual-uncased')\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Dropout(drop_rate),\n",
        "            nn.Linear(D_in, D_out))\n",
        "    def forward(self, input_ids, attention_masks):\n",
        "\n",
        "        outputs = self.bert(input_ids, attention_masks)\n",
        "        class_label_output = outputs[1]\n",
        "        outputs = self.regressor(class_label_output)\n",
        "        return outputs\n",
        "model = BertRegressor(drop_rate=0.2)\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU.\")\n",
        "else:\n",
        "    print(\"No GPU available, using the CPU instead.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=5e-5,\n",
        "                  eps=1e-8)\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 5\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                 num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "from torch.nn.utils.clip_grad import clip_grad_norm\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,\n",
        "          train_dataloader, device, clip_value=2):\n",
        "    for epoch in range(epochs):\n",
        "        print(epoch)\n",
        "        print(\"-----\")\n",
        "        best_loss = 1e10\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            print(step)\n",
        "            batch_inputs, batch_masks, batch_labels = \\\n",
        "                               tuple(b.to(device) for b in batch)\n",
        "            model.zero_grad()\n",
        "            outputs = model(batch_inputs, batch_masks)\n",
        "            loss = loss_function(outputs.squeeze(),\n",
        "                             batch_labels.squeeze())\n",
        "            loss.backward()\n",
        "            clip_grad_norm(model.parameters(), clip_value)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "    return model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs,\n",
        "              train_dataloader, device, clip_value=2)\n",
        "\n",
        "def evaluate(model, loss_function, test_dataloader, device):\n",
        "    model.eval()\n",
        "    test_loss, test_r2 = [], []\n",
        "    for batch in test_dataloader:\n",
        "        batch_inputs, batch_masks, batch_labels = \\\n",
        "                                 tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch_inputs, batch_masks)\n",
        "        loss = loss_function(outputs, batch_labels)\n",
        "        test_loss.append(loss.item())\n",
        "        r2 = r2_score(outputs, batch_labels)\n",
        "        test_r2.append(r2.item())\n",
        "    return test_loss, test_r2\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = torch.mean(labels)\n",
        "    ss_tot = torch.sum((labels - labels_mean) ** 2)\n",
        "    ss_res = torch.sum((labels - outputs) ** 2)\n",
        "    r2 = 1 - ss_res / ss_tot\n",
        "    return r2\n",
        "\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, _ = \\\n",
        "                                  tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs,\n",
        "                            batch_masks).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "val_set = val_data[['id_annonce', 'description', 'prix']]\n",
        "val_set['cleaned_description'] = \\\n",
        "                val_set.description.apply(clean_text)\n",
        "encoded_val_corpus = \\\n",
        "                tokenizer(text=val_set.cleaned_description.tolist(),\n",
        "                          add_special_tokens=True,\n",
        "                          padding='max_length',\n",
        "                          truncation='longest_first',\n",
        "                          max_length=300,\n",
        "                          return_attention_mask=True)\n",
        "val_input_ids = np.array(encoded_val_corpus['input_ids'])\n",
        "val_attention_mask = np.array(encoded_val_corpus['attention_mask'])\n",
        "val_labels = val_set.prix.to_numpy()\n",
        "val_labels = price_scaler.transform(val_labels.reshape(-1, 1))\n",
        "val_dataloader = create_dataloaders(val_input_ids,\n",
        "                         val_attention_mask, val_labels, batch_size)\n",
        "y_pred_scaled = predict(model, val_dataloader, device)\n",
        "\n",
        "y_test = val_set.prix.to_numpy()\n",
        "y_pred = price_scaler.inverse_transform(y_pred_scaled)\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import r2_score\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred))\\\n",
        "         / pd.Series(y_test)).abs().median()\n",
        "r_squared = r2_score(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "bhvcXKuXt8Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Your preprocessing functions and data loading code here...\n",
        "# ...\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased')\n",
        "\n",
        "# Assume df is your DataFrame and 'cleaned_description' is the text field\n",
        "encoded_corpus = tokenizer(\n",
        "    text=df.cleaned_description.tolist(),\n",
        "    add_special_tokens=True,\n",
        "    padding='max_length',\n",
        "    truncation='longest_first',\n",
        "    max_length=300,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "labels = df.prix.to_numpy()\n",
        "\n",
        "# Splitting the data\n",
        "test_size = 0.1\n",
        "seed = 42\n",
        "train_inputs, test_inputs, train_labels, test_labels = \\\n",
        "    train_test_split(input_ids, labels, test_size=test_size, random_state=seed)\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_mask, labels, test_size=test_size, random_state=seed)\n",
        "\n",
        "# Scaling the labels\n",
        "price_scaler = StandardScaler()\n",
        "price_scaler.fit(train_labels.reshape(-1, 1))\n",
        "train_labels = price_scaler.transform(train_labels.reshape(-1, 1))\n",
        "test_labels = price_scaler.transform(test_labels.reshape(-1, 1))\n",
        "\n",
        "# Define the model\n",
        "def build_model():\n",
        "    input_ids = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "    bert_output = bert_model([input_ids, attention_mask])\n",
        "    cls_token_output = bert_output.last_hidden_state[:, 0, :]\n",
        "    output = tf.keras.layers.Dense(1)(cls_token_output)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-8),\n",
        "                  loss=tf.keras.losses.MeanSquaredError(),\n",
        "                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x=[train_inputs, train_masks],\n",
        "    y=train_labels,\n",
        "    validation_data=([test_inputs, test_masks], test_labels),\n",
        "    batch_size=32,\n",
        "    epochs=5\n",
        ")\n",
        "\n",
        "# Evaluate and predict\n",
        "test_loss, test_mae = model.evaluate(x=[test_inputs, test_masks], y=test_labels)\n",
        "predictions = model.predict(x=[test_inputs, test_masks])\n",
        "\n",
        "# Rescale predictions\n",
        "y_pred = price_scaler.inverse_transform(predictions)\n",
        "# ...\n",
        "# Your evaluation code here\n",
        "# ...\n"
      ],
      "metadata": {
        "id": "YqI42aNq09fD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}